# Word2vec_embedding

Introduction to word2vec – Vector Representation of Words

We know that machines struggle to deal with raw text data. In fact, it’s almost impossible for machines to
deal with anything except for numerical data. So representing text in the form of vectors has always been the
most important step in almost all NLP tasks.

One of the most signi􀂦cant steps in this direction has been the use of word2vec embeddings, introduced to
the NLP community in 2013. It completely changed the entire landscape of NLP.

These embeddings proved to be state-of-the-art for tasks like word analogies and word similarities. word2vec
embeddings were also able to achieve tasks like King – man +woman ~= Queen, which was considered an
almost magical result.

Now, there are two variants of a word2vec model — Continuous Bag of Words and Skip-Gram model. In this
article, we will use the Skip-Gram model.
